---
title: "Working with Wikimedia Pageviews Data"
author: "David Pellon"
date: "Feb 29, 2020"
output: html_document
---
     
```{r section_date, include=TRUE, fig.width=15,fig.height=11}

knitr::opts_chunk$set(echo = TRUE)
Sys.time()

```
 
## Working with Wikimedia Pageviews Data
 
 This vignette shows how to scrap data from dump.wikimedia.org. 
 
 It is focused on wikimedia pageviews from *dumps.wikimedia.org*. 
 Wikimedia provides access to raw table dates on lists of compressed .gz files grouped in year-monthly web links.
 
 This work does not pursue any specific investigation or analysis but scraping the data, process it and plotting it.
 
 It is more about practicing skills than other thing.
 An important problem is the volume of data available. I perform some tricks to handle it, creating a few update procedures to avoid redownloading everything again, aswell as creating filters on domain or specific searches.
 
 Said that. Hope you find it interesting.
 
 
## Configuration
 
 I will be using knitr to publish the document on the web as a markdown document.
 
 I use multiple libraries listed below. I have a created a number of non very polished functions (due very restricted time limits)
 
 Additionally to *wikimedia* data, I have included *ISO codes* for country aggregations and *google trends* outputs.
 
 Constant definition:
     
     
```{r section_constants, include=TRUE, fig.width=15,fig.height=11}

rm(list = ls(all.names = TRUE)) #will clear all objects includes hidden objects.
gc() #free up memrory and report the memory usage.

library(sqldf)
library(cartography)
library(dplyr)
library(gtrendsR)
library(padr)
library(data.table)
library(plotrix)
library(ggplot2)
library(sp)
library(countrycode)

data(nuts2006)




# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# CONSTANTS
# ------------------------------------------------------------
#    CONST_WORKING_MONTH : (string)
#                dump.wikimedia.org datafiles folder
#    CONST_LOAD_FROM_DISK : (1/0)
#                load data from disk instead
#                                 of downloading it
#    CONST_SAVE_DOWNLOADED_DATA : (1/0)
#               will save the downloaded data for future use
#    CONST VERBOSITY: (0/1/2)
#               used for debugging
#               use higher value for more console output
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

CONST_VERBOSITY <- 1
CONST_WORKING_MONTH <- "2020-04"

# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# IMPORTANT !!!!
# ------------------------------------------------------------
# CONST_TOTAL_DAYS IS HARDCODED TO AVOID DOWNLOADING
# EVERY SINGLE DAY  AND OVERLOADING THE WEBSERVER
# LIMITING THE NUMBER OF DAYS TO DOWNLOAD TO 1
# ------------------------------------------------------------
CONST_TOTAL_DAYS <- 0
# set to 0 to disable hardcoding and use all days in the web month dir
# Or the value you need to force.
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

#```
# 
# 
# ## Functions
# 
# General functions:
#     
#```{r section_functions, include=TRUE, fig.width=15,fig.height=11}

count_DaysThisMonth <-
    function(month_Dir_df, verbose = CONST_VERBOSITY) {
        # --------------------------------------------------------------------------------------------
        # Receives a Dataframe with the day / hour available .gz files from dump.wikimedia.org
        # Returns:  total number of days
        # --------------------------------------------------------------------------------------------
        return(length(unique(month_Dir_df$days))) }

count_HoursThisDay <-
    function(month_Dir_df, this_Day, verbose = CONST_VERBOSITY) {
        # --------------------------------------------------------------------------------------------
        # Receives a Dataframe with the day / hour available .gz files from dump.wikimedia.org
        #               and the specific day with format "20200221"
        # Returns:  total number of hourly files with data for a given day
        # --------------------------------------------------------------------------------------------
        return(as.numeric(sqldf(paste("SELECT COUNT(days) FROM month_Dir_df WHERE days='",this_Day,"'",sep = ""))))}


get_MonthDir_df <-
    function(month_WebDir, verbose = CONST_VERBOSITY) {
        # --------------------------------------------------------------------------------------------
        # Receives a string with a month with the format "2020-02" like dump.wikimedia URL format
        # Returns:  a dataframe with the monthly days and hours with equivalent gz file with page views
        # --------------------------------------------------------------------------------------------
        table <- readLines(paste("https://dumps.wikimedia.org/other/pageviews/2020/",month_WebDir,"/",sep = ""))
        dtable <- as.data.frame(unlist(table))
        names(dtable) <- c("tags")
        page_views_df <- as.data.frame(dtable[grepl('<a href="pageviews-', dtable$tags), ])
        names(page_views_df) <- c("tags")
        foo <- data.frame(do.call("rbind", strsplit(as.character(page_views_df$tags), "-",fixed = TRUE)))[, c("X2", "X3")]
        foo$X1 <- data.frame(do.call("rbind", strsplit(as.character(foo$X3), ".",fixed = TRUE)))[, 1]
        foo$X3 <- NULL
        names(foo) <- c("days", "hours")
        return(foo)
    }

init_page_views_df <-
    function() {
        # --------------------------------------------------------------------------------------------
        # Initializes a dataframe with the structure below
        # --------------------------------------------------------------------------------------------  
        return(data.frame(domain_code = character(),
            page_title = character(),
            count_view = integer(),
            date = character(),
            hour = character(), stringsAsFactors = FALSE )) }

european_codes <- function()
{
    # --------------------------------------------------------------------------------------------
    # Returns a comma separated string with 3166-1 two character country domain codes
    # Used to be given as parameter to map plots for package cartography
    # --------------------------------------------------------------------------------------------  
    data(nuts2006)
    return(paste(tolower(nuts0.df$id), collapse = ","))
}
world_codes <- function() 
{
    # --------------------------------------------------------------------------------------------
    # Returns a comma separated string with 3166-1 two character country domain codes
    # Used to be given as parameter to map plots for package cartography
    # --------------------------------------------------------------------------------------------  

    paste(tolower(list(codelist$genc2c)),collapse='"')
    world_codes <- unlist(codelist$genc2c)
    world_codes[!is.na(world_codes)]
    return(gsub("NA,","",  paste(tolower(world_codes),collapse = ",")))
}


filter_Domains <-  function(page_views_df,filter_str, filter_out = 0, verbose = CONST_VERBOSITY) 
{
    # --------------------------------------------------------------------------------------------
    # Receives a Dataframe and a domain code filter string "en, es, fr". filter_out to 1 or 0
    # specifies if the domain codes should be the ones remaining or removed.
    # Returns:  Dataframe
    # --------------------------------------------------------------------------------------------
    page_views_df$domain_code  <- substr(page_views_df$domain_code, 1, 2)
    toMatch <- unlist(strsplit(as.character(filter_str), ",", fixed = TRUE))
    filtered_domain_codes <- unique(grep(paste(toMatch, collapse = "|"), page_views_df$domain_code,value = TRUE))
    if (filter_out == 0) { ret <- page_views_df[page_views_df$domain_code %in% filtered_domain_codes, ]
    } else {ret <- page_views_df[!page_views_df$domain_code %in% filtered_domain_codes, ] }
    return(ret)
}


get_Day_page_views_df <- function(month_Dir_df, this_Day, filter_str = "", filter_out = 0,  verbose = CONST_VERBOSITY) {
    # --------------------------------------------------------------------------------------------
    # Receives a Dataframe with the day / hour available .gz files from dump.wikimedia.org
    # Returns:  Dataframe with the page views for a given day
    #      "domain_code", "page_title", "count_views", "total_response_size", "date", "hour"
    # --------------------------------------------------------------------------------------------
    this_Year <- substr(this_Day, 1, 4)
    this_MonthDir <- paste(this_Year, "-", substr(this_Day, 5, 6), sep = "")
    # Initializing and empty day_data dataframe in which we will collect all the data in the loop
    page_views_Day_df <- init_page_views_df()
    count_Hours <- count_HoursThisDay(month_Dir_df, this_Day)
    
    for (k in 1:count_Hours)
    {
        this_Hour <- month_Dir_df$hours[k]
        pv_df_file = (paste("page_views_Hourly_df_",this_Day,"_",this_Hour,".rds",sep = ""))
        if (!file.exists(pv_df_file)) 
        {
            if (verbose == 1) print(paste("Downloading data for day: ", this_Day,", hour: ",this_Hour, sep = ""))
            url <- paste("https://dumps.wikimedia.org/other/pageviews/",this_Year,"/", this_MonthDir,"/pageviews-",this_Day,"-",this_Hour,".gz",sep = "")
            if (verbose == 2) print(paste("       Downloading data for hour - ", this_Hour," [", k,"/", count_Hours,"] -> ",url,sep = "")) 
            tmp <- tempfile()
            download.file(url, tmp)
            new_Data <- read.csv( gzfile(tmp), sep = " ", header = FALSE, stringsAsFactors = FALSE)
            names(new_Data) <- c( "domain_code","page_title","count_views","total_response_size" )
            new_Data <- as.data.frame(new_Data)
            new_Data$date <- this_Day
            new_Data$hour <- this_Hour
            if (verbose == 1) print(paste("               Saving data to -> ", pv_df_file, sep = ""))
            saveRDS(new_Data, file = pv_df_file)
        } else {
            if (verbose == 1) print(paste("Found local file for day: ", this_Day,", hour: ",this_Hour," . LOADING.", sep = ""))
            new_Data <- readRDS(file = pv_df_file)
        }
        if (filter_str != "") {
            before_Rows <- nrow(new_Data)
            new_Data <- filter_Domains(new_Data, filter_str, filter_out)
            after_Rows <- nrow(new_Data)
            if (verbose == 1) print(paste("Applied filter domain_code=",filter_str," ",ifelse(filter_out == 0, " - Inclusive ", " - Exclusive "),
                " - Reduced from : ", before_Rows," to ", after_Rows," rows", sep = "" ) ) }
        # We bind dataframe to dataframe by row to collect them all
        page_views_Day_df <- rbind(page_views_Day_df, new_Data)
    }
    return(page_views_Day_df)
}

total_views_ByPage <-
    function(page_views_df,
        TopN = 0,
        page_title_Search = "",
        verbose = CONST_VERBOSITY) {
        # --------------------------------------------------------------------------------------------
        # Receives a Dataframe with the day / hour available .gz files from dump.wikimedia.org
        # Returns:  total number of days
        # --------------------------------------------------------------------------------------------
        if (page_title_Search == "") {
            query_df <-
                sqldf(
                    "SELECT page_title, SUM(count_views) as total_views FROM page_views_df GROUP BY page_title ORDER BY total_views DESC"
                )
        } else {
            query_df <-
                sqldf(
                    paste(
                        "SELECT page_title, SUM(count_views) as total_views FROM page_views_df WHERE UPPER(page_title) LIKE UPPER('",
                        page_title_Search,
                        "' GROUP BY page_title ORDER BY total_views DESC",
                        sep = ""
                    )
                )
        }
        if (TopN > 0) {query_df <- head(query_df, TopN)}
        return(TopN)
    }


total_views_ByDate <- function(page_views_df, TopN = 0, page_title_Search = "", verbose = CONST_VERBOSITY) {
        # --------------------------------------------------------------------------------------------
        # Receives a Dataframe with the day / hour available .gz files from dump.wikimedia.org
        # Returns:  total number of days
        # --------------------------------------------------------------------------------------------
        if (page_title_Search == "") {
            query_df <-
                sqldf(
                    "SELECT page_title, SUM(count_views) as total_views FROM page_views_df GROUP BY date  ORDER BY total_views DESC"
                )
        } else {
            query_df <-
                sqldf(
                    paste(
                        "SELECT page_title, SUM(count_views) as total_views FROM date WHERE UPPER(page_title) LIKE UPPER('",
                        page_title_Search,
                        "' GROUP BY page_title ORDER BY total_views DESC",
                        sep = ""
                    )
                )
        }
        if (TopN > 0) {query_df <- head(query_df, TopN) }
    
        return(TopN)
    }


total_views_ByDomain <-
    function(page_views_df,
        TopN = 0,
        page_title_Search = "",
        verbose = CONST_VERBOSITY) 
        {
        # --------------------------------------------------------------------------------------------
        # Receives a Dataframe with the day / hour available .gz files from dump.wikimedia.org
        # Returns:  total number of days
        # --------------------------------------------------------------------------------------------
        if (page_title_Search == "") {
            query_df <-
                sqldf(
                    "SELECT domain_code, SUM(count_views) as total_views FROM page_views_df GROUP BY domain_code ORDER BY total_views DESC"
                )
        } else {query_df <- sqldf(paste("SELECT domain_code, SUM(count_views) as total_views FROM page_views_df WHERE UPPER(page_title) LIKE UPPER('",
                        page_title_Search,"') GROUP BY domain_code ORDER BY total_views DESC", sep = "")) }
        if (TopN > 0) {query_df <- head(query_df, TopN) }
        return(query_df)
    }

prepare_page_views_Day_df <- function(this_Day,domain_filter_string = "", verbose = CONST_VERBOSITY, Action = 0)
{
    # --------------------------------------------------------------------------------------------
    # Receives: 
    #    + this_Day: Day string in the format "20200203" 
    #    + domain_filter_string: contains a serie of domain codes. i.e: "en,es,fr,de"
    #              Also accepts "world_codes" and "european_codes"
    #    + Action:  0 - Normal Behaviour
    #               1 - Only download when needed. Do not load
    #
    # Returns:  Dataframe with the page views for a given day
    #      "domain_code", "page_title", "count_views", "total_response_size", "date", "hour"
    #
    # It tries to find the data in disk, if not available it will download it, filter it and 
    #       save it to disk
    # --------------------------------------------------------------------------------------------
    i_codes_str <- function(domain_filter_string)
    {
        ret = domain_filter_string
        if (tolower(domain_filter_string) == tolower("world_codes")) ret = world_codes()
        if (tolower(domain_filter_string) == tolower("european_codes")) ret = european_codes()
        return(ret)
    }
    month_Dir_df <- get_MonthDir_df(CONST_WORKING_MONTH,verbose)
    page_views_Day_df <- init_page_views_df()
    pv_domfilt_df_str = paste("page_views_Day_df_",this_Day,"_fs_",domain_filter_string,sep = "")
    pv_domfilt_df_file = paste(pv_domfilt_df_str,".rds",sep = "")
    if (file.exists(pv_domfilt_df_file)) 
    {
        if (Action == 1)
        {
            if (verbose == 1) print(paste("FILE FOUND (",pv_domfilt_df_file,". Action set to only download when needed. NOT LOADING -> SKIPPING",sep = ""))
        } else  {
            if (verbose == 1) print(paste("Found local file (",pv_domfilt_df_file,") for day (",this_Day,") and this domain filter string (",domain_filter_string,"). LOADING ",sep = ""))
            page_views_Day_df <- readRDS(file = pv_domfilt_df_file) 
            if (verbose == 1) print(paste("LOADED INTO VARIABLE > page_views_Day_df",sep = ""))
        } 
    } else  {
        if (verbose == 1) print(paste("Warning: Could not find any local file for day (",this_Day,") and this domain filter string (",domain_filter_string,").",sep = ""))
        pv_df_str = gsub(paste("_fs_",domain_filter_string,sep = ""),"",pv_domfilt_df_str)
        pv_df_file = paste(pv_df_str,".rds",sep = "")
        if (verbose == 1) print(paste("         Looking for generic full day file ",pv_df_file,". LOADING >> Filtering >> Saveing. Will take longer",sep = ""))
        if (file.exists(pv_df_file)) 
        {
            if (verbose == 1) print(paste("         Found local file (",pv_df_file,") for day (",this_Day,"). LOADING ",sep = ""))
            page_views_Day_df <- readRDS(file = pv_df_file) 
            if (verbose == 1) print(paste("         Filtering it now to given domain codes -> ",domain_filter_string,sep = ""))
            page_views_Day_df <- filter_Domains(page_views_Day_df, i_codes_str(domain_filter_string), filter_out = 0, verbose) 
            if (verbose == 1) print(paste("         We will save the filtered version -> ",domain_filter_string,sep = ""))
            saveRDS(page_views_Day_df,file = pv_domfilt_df_file)
            if (verbose == 1) print(paste("         SAVED ! ",sep = ""))
        } else {
            if (verbose == 1) print(paste("         Warning: Could not find any local file for day (",this_Day,") either",sep = ""))
            if (verbose == 1) print("         DOWNLOADING >> Filtering >> Saving. Will take LONGER")
            page_views_Day_df <- get_Day_page_views_df(month_Dir_df, this_Day, filter_str = i_codes_str(domain_filter_string), filter_out = 0)
            if (verbose == 1) print(paste("         We will save the filtered version -> ",pv_domfilt_df_file,sep = ""))
            saveRDS(page_views_Day_df,file = pv_domfilt_df_file)
            if (verbose == 1) print(paste("         SAVED ! ",sep = ""))
        }
    }
    return(page_views_Day_df)
}  

map_plot_page_views_Europe <- function(page_views_df,
                                       main_Title = "",
                                       use_Log_Scale = 0,
                                       start_Color = "black",
                                       end_Color = "orange",
                                       annotation_Author = "",
                                       annotation_Sources = "",
                                       page_title_Search = "",
                                       legend = "",
                                       verbose = CONST_VERBOSITY) {
  
  # page_views_df <- page_views_Month_df
  # use_Log_Scale = 0
  # head(european_domain_codes_df)
  # --------------------------------------------------------------------------------------------
  # Receives a page views dataframe
  #      - optional - page_title_Search paramter
  #                 - use_log_scale for visibility & clarity when extreme differences
  #                 - other presentation options
  # Plots an european map with the relative weight of total page views
  # --------------------------------------------------------------------------------------------

  # We will use the european domain codes from nuts2006
  data(nuts2006)
  european_codes <- paste(tolower(nuts0.df$id), collapse = ",")
  page_views_df$domain_code <- substr(page_views_df$domain_code, 1, 2)
  if (page_title_Search == "") {
    total_views_by_domain <-
      sqldf(
        "SELECT domain_code, SUM(count_views) as total_views FROM page_views_df GROUP BY domain_code ORDER BY total_views DESC"
      )
  } else {
    total_views_by_domain <-
      sqldf(
        paste("SELECT domain_code, SUM(count_views) as total_views FROM page_views_df WHERE UPPER(page_title) LIKE UPPER('", page_Title_Search, 
            "' GROUP BY domain_code ORDER BY total_views DESC"))
    main_Title <- paste(main_Title, " - Search ", page_title_Search, sep = "")
  }
  if (use_Log_Scale == 1) {total_views_by_domain$total_views <- log(total_views_by_domain$total_views)}
  european_domain_codes_df <- filter_Domains(total_views_by_domain,filter_str = european_codes, filter_out = 0 )
  names(european_domain_codes_df) <- c("id", "total_views")
  european_domain_codes_df$id <- toupper(european_domain_codes_df$id)
  
  # The map plot library needs to have all the country codes filled, even when zero
  missing_domains <- nuts0.df[!nuts0.df$id %in% european_domain_codes_df$id, ][1]
  missing_domains$total_views <- 0
  european_domain_codes_df <- rbind(missing_domains, european_domain_codes_df)
  european_domain_codes_df$total_views <- as.double(european_domain_codes_df$total_views)
  plot(nuts0.spdf, col = start_Color, add = FALSE)  
  propSymbolsLayer(spdf = nuts0.spdf,df = european_domain_codes_df, var = "total_views", symbols = "circle", col = end_Color,
                   legend.pos = "right", legend.title.txt = legend, legend.style = "c") 
  layoutLayer(title = main_Title, author = annotation_Author, sources = annotation_Sources, scale = TRUE, south = TRUE )
  return()
}
    
query_aggregate_daily_to_monthly <- function(search_String = "",count_Days = 1, month_Dir_df ,domain_filter_string = "", domain_filter_exclusivity = 0 , verbose = CONST_VERBOSITY , save_filename = "")
{
    # --------------------------------------------------------------------------------------------
    # Aggregates every daily page_view dataframe in disk into a monthly one
    # --------------------------------------------------------------------------------------------
    # Receives:
    #     - search_String = query to filter by page titles 
    #     - count_Days = number of days in the month 
    #     - domain_filter_string = query to filter by domain code (ISO 3166-1 two char country codes
    #             (should be provided as a comma separated string) i.e. : "en,es,fr,de"
    #     - CONST_VERBOSITY = Will log more information to the console
    #     - save_filename - a specific file name can be provided or it will create one automatically
    # --------------------------------------------------------------------------------------------  
    page_views_Month_df <- init_page_views_df()
    for (k in 1:count_Days)
    {
        daily_df <- init_page_views_df()
        this_Day <- unique(month_Dir_df$days)[k]
        if (verbose == 1) {print(paste("Aggregating day ",k," of ",count_Days,sep = ""))}
        daily_df <- prepare_page_views_Day_df(this_Day,domain_filter_string,Action = 0)
        if (search_String != "")
        {
            rows_before_filter = nrow(daily_df)            
            daily_df %>% filter(toupper(page_title) == gsub(" ","_",toupper(search_String))) -> daily_df
            rows_after_filter = nrow(daily_df)
            if (verbose == 1) print(paste("          (rows filtered from ",rows_before_filter," to ",rows_after_filter,")", sep = ""))            
        }
        page_views_Month_df <- rbind(page_views_Month_df, daily_df )
        if (verbose == 1) {print(paste("Aggregating date = '",this_Day,"'  daily_rows = ",nrow(daily_df)," to monthly existing rows = ", nrow(page_views_Month_df),sep = "") ) }
    }
    if (save_filename == "")
    {
        output_name = paste("query_",gsub(" ","_",search_String),"_",unique(month_Dir_df$days)[1],"-",unique(month_Dir_df$days)[count_Days],sep = "")
    } else {output_name = save_filename }
    if (verbose == 1) {print(paste("Saved to ",output_name,sep = "") ) }
    saveRDS(page_views_Month_df, file = paste(output_name,".Rds",sep = ""))
    return(page_views_Month_df)
}
    
search_String_ByDate <- function(search_String, df) 
{
    ret = aggregate(df$count_views,FUN = sum,by = list(df$date))
    names(ret) <- c("date","total_hits")
    return(ret)
}
    
AddGoogleTrendsSearchToDf = function(df,search_String)
{
    replace_na_with_last <- function(x,a=!is.na(x)){
        x[which(a)[c(1,1:sum(a))][cumsum(a) + 1]]
    }
    gtdata <- gtrends(search_String,time = "today 1-m")$interest_over_time
    #We fill all missing dates at day timeframe (every single day)
    gt <- pad(gtdata,interval = "day")
    gt$hits <- gsub("<","",gt$hits)
    gt <- pad(gtdata,interval = "day")
    gt$hits <- as.numeric(gt$hits)
    gt$hits <- nafill(gt$hits, type = "locf")
    gt[,3:7] <- NULL
    gt$date <- as.numeric(as.character(gsub("-","",gt$date)))
    df$date <- as.numeric(as.character(df$date))
    date_range <- seq(min(df$date),max(df$date),by = 1)
    exception1 <- date_range[!date_range %in% gt$date]
    date_range <- seq(min(gt$date),max(gt$date),by = 1)
    exception2 <- date_range[!date_range %in% df$date]
    gt <- gt[!gt$date %in% exception2,]
    replace_na_with_last(gt$gtrends_hits_Coronavirus)
    df <- merge(df,gt,all.x = TRUE, by.x = c("date"),by.y = c("date"))
    names(df)[names(df) == "hits"] <- paste("gtrends_hits_",gsub(" ","_",search_String),sep = "")
    return(df)
}     

circular_bar_plot <- function(twoColumn_df, title = "",xtext = "", ytext = "", use_log_scale = 0, startcolor = "black", endcolor = "orange",  use_scale_width = 0, verbose = CONST_VERBOSITY) 
    {
    # --------------------------------------------------------------------------------------------
    # Receives a two column dataframe with labels and numbers
    # Displays a Circular bar plot
    # --------------------------------------------------------------------------------------------

    chart_Data <- data.frame(
      id = numeric(),
      angle = numeric(),
      hjust = numeric(),
      value = numeric(),
      label = character(),
      stringsAsFactors = FALSE
    )
    max_LengthAllowedInLabel <- 25
    min_LabelTextSize <- 5
    max_LabelTextSize <- 10
    number_of_bars <- nrow(twoColumn_df)
    for (i in 1:number_of_bars)
    {
      chart_Data[i, 1] <- i
      chart_Data[i, 2] <- 90 - 360 * (i - 0.5) / number_of_bars
      chart_Data[i, 3] <- ifelse(chart_Data[i, 2] < -90, 1, 0)
      chart_Data[i, 2] <- ifelse(chart_Data[i, 2] < -90, chart_Data[i, 2] + 180, chart_Data[i, 2])
      chart_Data[i, 4] <- ifelse(use_log_scale == 1, log(twoColumn_df[i, 2]), twoColumn_df[i, 2])
      chart_Data[i, 5] <- gsub("_"," ",substr(twoColumn_df[i, 1], 1, max_LengthAllowedInLabel))
    }
    chart_Data <- as.data.frame(chart_Data)

    # Rescaling the label text size depending on the string length. With a @hardcoded maximum of 20 characters
    max_StringLengthInColumn <- max(nchar(chart_Data$label, type = "chars"), na.rm = TRUE)

    label_TextScaling <- min(round(max_LengthAllowedInLabel / max_StringLengthInColumn * min_LabelTextSize,0), max_LabelTextSize)

    p <- ggplot(chart_Data, aes(x = as.factor(id), y = value, fill = value)) +
      geom_bar(stat = "identity",width = ifelse(use_scale_width == 1,0.9 * (chart_Data$value / chart_Data$value[1])^3,0.9)) +
      scale_fill_continuous(low = startcolor, high = endcolor) + ylim(-chart_Data[1, 4], chart_Data[1, 4] * 1.1) +
      coord_polar(start = 0) +    
        geom_text(
        data = chart_Data,
        aes(x = id,y = value * 1.01,label = label,hjust = hjust ),
        color = "black",
        fontface = "bold",
        alpha = 0.6,
        size = label_TextScaling,
        angle = chart_Data$angle,
        inherit.aes = FALSE,
        nudge_y = 6
      ) + labs(x = xtext, y = ytext) + ggtitle(paste(title, ifelse(use_log_scale == 1, " (Log Scale)", ""), sep = ""))
    p
  }

    
    
```
 
 
## Data Processing
 
We scrap from *dump.wikimedia.org* a list for a given month with every day and their hourly .gz files with their compressed page view statistics.
 
Those files are later downloaded, uncompressed, stored, filtered and aggregated into a bigger dataframe.

Then some plots and charts are created.
 

*NOTE:* 
I have *hardcoded total_days with the CONST_TOTAL_DAYS* override (at the top) to avoid looping through all the month as it is hugely cost intensive in time and memory.
    
    
```{r section_Data_Processing, include=TRUE, fig.width=15,fig.height=11}  
    
month_Dir_df <- get_MonthDir_df(CONST_WORKING_MONTH)
count_Days <- ifelse(CONST_TOTAL_DAYS > 0,CONST_TOTAL_DAYS,  count_DaysThisMonth(month_Dir_df))
    
if (CONST_VERBOSITY == 1 &  CONST_TOTAL_DAYS > 0) {
    print(paste("V1 > Number of days is hardcoded in code to ",CONST_TOTAL_DAYS, ". Only those will be used. Total available days for this month would be ",
    count_DaysThisMonth(month_Dir_df),sep = "") )}
if (CONST_VERBOSITY == 1) {print(paste("V1 > Number of days to obtain - ", count_Days, sep = ""))}
    
    page_views_Month_df <- init_page_views_df()
    
page_views_Month_df<-query_aggregate_daily_to_monthly("",count_Days = 2, month_Dir_df ,domain_filter_string = "european_codes")

```    

## Data Exploration

Now that we have dowloaded the data we can do some initial exploration of the data

The Top 10 Domains with the highest number of page views for the day with *european domain codes*:

```{r section_total_views_ByDomain, include=TRUE, fig.width=15,fig.height=11}

#summary(page_views_Month_df)

total_views_ByDomain_df <- total_views_ByDomain( page_views_df = page_views_Month_df,TopN = 15, page_title_Search = ""  )
head(total_views_ByDomain_df,10)

```

A basic pie chart would be:

```{r section_total_views_ByDomain_pie, include=TRUE, fig.width=15,fig.height=11}

ini_color = "white"
end_color = "black"

cols <- colorRampPalette(c(ini_color,end_color))(length(total_views_ByDomain_df$domain_code))

pie3D(total_views_ByDomain_df$total_views,labels = total_views_ByDomain_df$domain_code, explode = 0.085, shade = 0.7, theta = 1, radius = 0.9, 
  main = "Total Wikimedia Page Views by Top European Continental Domain Codes",col=c(cols))

```

A relative weight representation of total page views grouped by European Domain Codes (ISO 3166-1)


```{r section_total_views_ByDomain_map_plot, include=TRUE, fig.width=15,fig.height=11}


map_plot_page_views_Europe(
  page_views_Month_df,
  main_Title  = "Wikimedia Total Page Views by Domain Code (ISO 3166-1) in the European Region",
  annotation_Author = "David Pellon",
  annotation_Sources = "Wikimedia page views",
  legend = "Total Page Views",
  page_title_Search = "",
  start_Color=ini_color,
  end_Color = end_color
)

```



Now the 10 top Pages Titles with highest number of page views:

```{r top10PageTitles_byPage, include=TRUE, fig.width=15,fig.height=11}


page_views_ByPage <-
  total_views_ByPage(
    page_views_df = page_views_Month_df,
    TopN = 10,
    page_title_Search = ""
  )


```

And the pie chart would be :

Note: As it won't be pretty with such long page titles, I will display only the first 20 (?) characters.

```{r section_page_views_ByPage_circular_bar_plot, include=TRUE, fig.width=15,fig.height=11}

circular_bar_plot(page_views_ByPage, use_log = 0, title = paste("Top 10 Page Links Aggregated Page Views for ",this_Day,sep = "", use_log_scale = 1 )) 

```


Can we see what countries pay more attention nowadays to link "CoronaVirus" ?

```{r section_total_views_ByDomain_Search_Coronavirus, include=TRUE, fig.width=15,fig.height=11}

total_views_ByDomain_Search_Coronavirus <- total_views_ByDomain( page_views_df = page_views_Month_df,TopN = 10, page_title_Search = "Coronavirus"  )

circular_bar_plot(
  total_views_ByDomain_Search_Coronavirus,
  use_log = 0,
  title = paste(
    "Top 10 Domain Codes Page Views Searching in Wikimedia for 'Coronavirus'",
    this_Day,
    sep = "",
    use_log_scale = 1
  )
)
```

How is the geographical interest distribution on European countries?


```{r section_, include=TRUE, fig.width=15,fig.height=11}
monthly_coronavirus <- query_aggregate_daily_to_monthly(search_String = "coronavirus",count_days,month_Dir_df,domain_filter_string = european_codes() ,domain_filter_exclusivity = 0 ,CONST_VERBOSITY = 1)

map_plot_page_views_Europe(
  monthly_coronavirus,
  title = "Wikimedia Total 'Coronavirus' Page Views by Domain Code (ISO 3166-1) in the European Region",
  author                    = "aceri",
  sources                   = "Wikimedia",
  legend                    = "Total Page Views",
  page_title_Search         = ""
)
```



    
Are the results different from those of Google Trends. Let's aggregate the full month, filter it by 'en' domain code and group them by date and correlate them with Google Trends data for the last month.

Seems the results are pretty similar. The Coronavirus in Google Trends received a great peak after the 20th.
    
``` {r section_ComparingToGoogleTrends, include=TRUE, fig.width=15,fig.height=11}      
    
    page_views_Month_Q_Coronavirus_df <-    readRDS(file="query_Coronavirus_20200201-20200226.Rds")
    page_views_Month_Q_Kobe_Bryant_df <-    readRDS(file="query_Kobe_Bryant_20200201-20200226.Rds")

    agr_Q_Coronavirus_ByDate <- search_String_ByDate("Coronavirus" , page_views_Month_Q_Coronavirus_df) 
    agr_Q_Kobe_Bryant_ByDate <- search_String_ByDate("Kobe Bryant" , page_views_Month_Q_Kobe_Bryant_df) 

    gtHits_Q_Coronavirus_df <- AddGoogleTrendsSearchToDf(agr_Q_Coronavirus_ByDate        ,"Coronavirus")
    gtHits_Q_Kobe_Bryant_df <- AddGoogleTrendsSearchToDf(agr_Q_Kobe_Bryant_ByDate        ,"Kobe Bryant")

    par(mfrow=c(2,2))
    plot(gtHits_Q_Coronavirus_df$total_hits , type ="l", xlab="February",ylab="Total Page Views", main="Total Page Views for 'Coronavirus' in 'en' Domain")
    plot(gtHits_Q_Kobe_Bryant_df$total_hits , type ="l", xlab="February",ylab="Total Page Views", main="Total Page Views for 'Kobe Bryant' in 'en' Domain")
    plot(gtHits_Q_Coronavirus_df$gtrends_hits_Coronavirus , type ="l",xlab="February",ylab="Google Trends Scale", main="Google Trends for 'Coronavirus' - Worldwide 1 Month")
    plot(gtHits_Q_Kobe_Bryant_df$gtrends_hits_Kobe_Bryant , type ="l",xlab="February",ylab="Google Trends Scale", main="Google Trends for 'Kobe Bryant' - Worldwide 1 Month")

```
## Observations

Very interesting and useful open data from *wikimedia*. 

But sometimes the good things do not come alone. I have found quite problematic to handle such amount of data with R.
Queries are very cost intensive and would certainly benefit from parallel map reduction.

This has been a quick draft scraping & processing experiment from some wikipedia data. 

They provide very rich and diverse statistic (and contribution!) resources like:

[Wikistats Contributing & Deployment](https://wikitech.wikimedia.org/wiki/Analytics/Systems/Wikistats_2#Contributing_and_Deployment)
[Wikimedia All Projects](https://stats.wikimedia.org/index.html#/all-projects)
[Github Analytics Wikistats](https://github.com/wikimedia/analytics-wikistats2/search?o=desc&q=release&s=committer-date&type=Commits)


A more thorough examination would be needed to decide on the best tools and data sources for any desired outcome.

     ,.
 __.'_
|__|__|
|     |      
|-___-|
'.___.' DP


    
    
    